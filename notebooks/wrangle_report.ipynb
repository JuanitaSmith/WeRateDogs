{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue; font-size:30px; line-height:40px; font-family: Calibri;\">Project 3: Data wrangling report</span>\n",
    "\n",
    "<img src=\"../images/weratedogs.png\" alt=\"drawing\" width=\"500\" style=\"float:left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue; font-size:26px; line-height:40px; font-family: Calibri;\">Project Overview</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the project was to demonstrate data wrangling and analysis skills obtained from the course.\n",
    "\n",
    "The dataset to be wrangled is the tweet archive of Twitter user [@dog_rates](https://twitter.com/dog_rates), also known as [WeRateDogs](https://en.wikipedia.org/wiki/WeRateDogs). WeRateDogs is a Twitter account that rates people's dogs with a humorous comment about the dog. Dog owners upload a picture of their dog, writes a funny comment and give their own dog a rating out of 10. Owners almost always give their dogs a rating greater than 10. e.g 11/10, 12/10, 13/10, it's the nature of the website. WeRateDogs has over 4 million followers and has received international media coverage. It's only a bit of fun, and brings great joy when browsing the cutest dog images !\n",
    "\n",
    "\n",
    "### Data wrangling consist of 3 main steps:\n",
    ">#### **Gather -> Assess -> Clean**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue; font-size:26px; line-height:40px; font-family: Calibri;\">Wrangling Step 1: Data gathering</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real-world data rarely comes clean, especially when it comes from the internet where users enter data into free text fields that are optional, as it's the case with this twitter account.\n",
    "\n",
    "During the data gathering step, twitter data needed to be downloaded **automaticaly** from the internet using a variety of sources and techniques, with the help of Python and its libraries.\n",
    "\n",
    "3 data gathering strategies was demonstrated:\n",
    "\n",
    "### 1. Twitter archive - Automated downloaded from an URL\n",
    "\n",
    "WeRateDogs downloaded their Twitter archive and sent it to Udacity via email exclusively to use in this project. This archive contains basic tweet data (tweet ID, timestamp, text, etc.) for a sample of 2356 tweets with ratings, for a period between Nov 2015 - July 2017.\n",
    "Data was automatically downloaded from the Udacity website using `pd.read_csv` and passing the URL as input.\n",
    "\n",
    "### 2. Use twitter API 'tweepy v2' to retrieve additional data\n",
    "\n",
    "For each tweet in the twitter archive from step 1, the retweet and like count was retrieved as minimum requirement for the project\n",
    "\n",
    "The latest version of the tweepy v2 API was used to retrieve the data, which offered more flexibility and data availability, although it took some time to figure out how to use it. I initially tried to download data like geo data, country, place, additional metrics etc, as these could make interesting analytics, however this data was never filled.\n",
    "\n",
    "Each tweet's entire JSON response was dumped to a txt file, each tweet on its own line\n",
    "\n",
    "The .txt file was read line by line, and the most useful data was added to a pandas dataframe. Some API data were already present in twitter archive from step 1, however might be useful to compliment missing or corrupt data potentially. \n",
    "\n",
    "Some API errors occurred which was dumped to txt log file. Some tweets were no longer availabe in the API.\n",
    "\n",
    "\n",
    "### 3. Use the Requests library to download the tweet breed predictions (image_predictions.tsv)\n",
    "\n",
    "Udacity provided a file that used the dog images to predict the breed of the dog.\n",
    "Each tweet have a URL to the image that gave the best breed predictions, 3 predictions each with it's confidence level and if the prediction is actually a valid dog breed.\n",
    "\n",
    "This file (image_predictions.tsv), is hosted on Udacity's servers and was downloaded programmatically using the Requests library and the following URL: https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\n",
    "\n",
    "\n",
    "### 4. Download dog profile images (optional extra step)\n",
    "\n",
    "Image urls containing `.jpg` retrieved from API in step 2 was used to download the first dog image for each tweet from the internet, as additional practice. I had in mind for visualization to show a dog profile card of the most liked breeds and dogs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue; font-size:26px; line-height:40px; font-family: Calibri;\">Wrangling Step 2: Assessing</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** \n",
    "Data obtained are indeed very untidy. The requirements of this project are only to assess and clean at least 8 quality issues and at least 2 tidiness issues to demonstrate data wrangling skills.\n",
    "Even though I went beyond this requirement, still not all issues are assessed and cleaned to perfection.\n",
    "The data were assessed and cleaned, upon which the analysis and visualizations will be based."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assessing was done using first visual inspection, followed by programmatic inspection for each of the 3 datasets acquired in step 1.\n",
    "Visual inspection was done by manually inspecting the data after displaying a sample of 10-20 records, using `.sample()`\n",
    "Programmatic inspection was done using .`info(), nunqiue(), unique(), .duplicated()`. Deeper inspection was done where problems were spotted, using filtering and query techniques.\n",
    "\n",
    "During the assessment stage, the 'what' is documented, describing only the **observed** problem. The problems were organized by quality vs tidiness issues.\n",
    "\n",
    "Quality issues were sub organized by completeness, validity, accuracy and consistency, and sorted in order of importance, the same order that will be followed during the cleaning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/assessment.png\" alt=\"assessment\" width=\"600\" style=\"float:left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue; font-size:26px; line-height:40px; font-family: Calibri;\">Wrangling Step 3: Cleaning</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, copies were made of each dataset, which was further used to perform the cleaning actions on.\n",
    "\n",
    "Cleaning was done following the framework **Define -> Code -> Test** for each issue found during assessing one-by-one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "\n",
    "<img src=\"../images/cleanexample.png\" alt=\"cleanexample\" width=\"900\" style=\"float:left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the cleaning step, we have a new master twitter archive dataset exported as `.csv` file, containing no duplicate columns, with the best data merged from all 3 datasets.\n",
    "Only original tweets with images were kept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/masterstructure.png\" alt=\"datastructure\" width=\"400\" style=\"float:left\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
